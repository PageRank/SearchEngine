\documentclass[pdftex,12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[lined,boxed,linesnumbered,commentsnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{hyperref}


\begin{document}

\title{
	\Large \textbf{Algorithms for social networks} \\
	\textsf{Report of the project \emph{PageRank}}
}
\author{Andreea Beica \and Baptiste Lefebvre}
\date{21 januar 2014}

\maketitle


\section{Presentation}

The PageRank algorithm, alongside a family of Google related technologies, represent a unique approach to the task of information handling and data retrieval, their scope and possible applications extending far beyond the search box of Google.com.

More specifically, PageRank is a method of assessing the importance of a web page based upon its relationship with to other pages. As presented in the Abstract of the original patent, it is :

\vspace{10pt}

\begin{centering}
\textbf{"A method [that] assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database."}
\end{centering}
\vspace{10pt}

What makes PageRank's importance extend beyond the scope of a simple search engine, is that when applied to all of the web pages within the Google index and after analysing the output, it constitutes an accurate of user behavior probability, as well as offering a completely objective measure measure of a web page's importance, all of which is computed mathematically and without human interference. 

In the words of its authors:

\vspace{10pt}

\begin{centering}
\textbf{"PageRank can be thought of as a model of user behavior. We assume there is a \emph{random surfer} who is given a web page at random and keeps clicking on links, never hitting \emph{back} but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank."}
\end{centering}
\vspace{10pt}

When compared to already existing text-based search engines, which relied primarily on information contained within given page (such as titles with a high keyword density, search terms placed near the top of the document, or having the highest density of the search term) to determine its relevance to a search query - and so which could be easily manipulated and ultimately unreliable, the PageRank approach aimed to provide a vast improvement in the quality and accuracy of document retrieved by a search engine in response to a search  phrase query. This goal would be accomplished by:

\begin{itemize}
\item objectively evaluating the importance of web pages and assigning corresponding ranks
\item using the rank information of said web pages to help determine relevance and placement withing a Search Engine Result Page
\end{itemize}

The formal description of the PageRank formaula is as follows (as described by its authors):

\vspace{10pt}


\textbf{"We assume page A has pages T1...Tn which point to it (i.e., are citations).
The parameter d is a damping factor which can be set between 0 and 1. We
usually set d to 0.85.
Also C(A) is defined as the number of links going out of page A. The
PageRank of a page A is given as follows:}

\vspace{10pt}

\begin{centering}
\textbf{PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))}
\end{centering}
\vspace{10pt}

\textbf{Note that the PageRanks form a probability distribution over web pages, so the sum of all web pages' PageRanks will be one."}

\vspace{10pt}

We break down the formula of PR(A), for clarification purposes:

\begin{itemize}
\item \textbf{PR(Ti)} = the PageRank of $i^{th}$ page pointing to page A
\item \textbf{PR(Ti)/C(Ti)} = any linking web page divides the weight of its vote evenly amongst all of the votes that it gives
\item \textbf{d} = every vote is added together to determine the PageRank value of page A; however, in order to prevent a \emph{combined strength} effect from over inflating PR(A), the final value is multiplied by a \emph{damping factor}, of usually .85
\item \textbf{(1-d)} = this ensures that the sum of all web pages' PageRank will be 1, by adding the amount lost by the damping factor back in ; thus, every web page will have at least that minimal value
\end{itemize}

The purpose of this project is to implement different versions of the PageRank and test it on a set of Wikipedia set of downloaded pages.


\section{Project architecture}

Bla, bla, bla...


\section{Algorithms}

Three methods have been selected to implement the PageRank algorithm:
\begin{itemize}
\item iterative method
\item algebraic method
\item power method
\end{itemize}
This section details each of this method. They are all three based on the same input, a set $P = p_1, p_2, ..., p_n$ of HTML pages, with some hyperlinks between pages (i.e. $p_i \rightarrow p_j$), which defines a webgraph $G = \left(V, E\right)$ such that:
$$V = \{i \in N / p_i \in P\}$$
and:
$$E = \{\left(i, j\right) \in V \times V / p_i \rightarrow p_j\}$$
To make notations convenient, the following concepts are useful:
\begin{itemize}
\item $M\left(v\right)$ denotes the set of vertices that link to $v$ in $G$
\item $L\left(b\right)$ denotes the number of outbound edges for node $v$ in $G$
\item $A$ denotes the adjacency matrix of $G$ (i.e. $A_G\left[i\right]\left[j\right] \in \{0, 1\}$ and $A_G\left[i\right]\left[j\right] = 1 \Leftrightarrow \left(i, j\right) \in E$)
\end{itemize}

\subsection{Iterative method}
\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\BlankLine
\Indm
\Input{A webgraph $G$ with $n$ nodes}
\Output{An array $pr$ of size $n$}
\Indp
\BlankLine
\emph{// Initialization}\;
\For{$i \leftarrow 1$ \KwTo $n$}{
$pr'\left[i\right]\leftarrow\infty$\;
$pr\left[i\right]\leftarrow\frac{1}{n}$\;
}
\emph{// Iterations}\;
\While{$|pr-pr'|\geq\epsilon$}{
$pr' \leftarrow pr$\;
\For{$i \leftarrow 1$ \KwTo $n$}{
$pr\left[i\right]\leftarrow\frac{1-d}{N}+d\sum_{j\in M\left(i\right)}\frac{pr'[j]}{L\left(j\right)}$\;
}
}
\emph{// Renormalization}\;
$pr\leftarrow\frac{pr}{|pr|}$\;
\BlankLine
\caption{Iterative methods}\label{algo_iterative_method}
\end{algorithm}
\DecMargin{1em}

\subsection{Algebraic method}
Additional notations have been used to describe the algebraic method, for a given integer $n$:
\begin{itemize}
\item $I$ denotes the identity matrix of size $n$
\item $O$ denotes the column vector of length $n$ containing only ones
\end{itemize}

\IncMargin{1em}
\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\BlankLine
\Indm
\Input{A adjacency matrix $A$ of a webgraph $G$ with $n$ nodes}
\Output{An array $pr$ of size $n$}
\Indp
\BlankLine
\emph{// Initialisation}\;
\For{$j \leftarrow 1$ \KwTo $n$}{
\If{$L\left(j\right) \neq 0$}{
\For{$i \leftarrow 1$ \KwTo $n$}{
$A\left[i\right]\left[j\right]\leftarrow \frac{A\left[i\right]\left[j\right]}{L\left(j\right)}$
}
}
}
\emph{Computation}\;
$pr\leftarrow \left(dA-I\right)^{-1}\frac{1-d}{n}O$
\BlankLine
\caption{Algebraic method}\label{algebraic_method}
\end{algorithm}
\DecMargin{1em}


\subsection{Power method}
\IncMargin{1em}
\begin{algorithm}[H]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\BlankLine
\Indm
\Input{A webgraph $G$ with $n$ nodes}
\Output{An array $pr$ of size $n$}
\Indp
\BlankLine
\emph{// Initialization}\;
\For{$i \leftarrow 1$ \KwTo $n$}{
$pr'\left[i\right]\leftarrow\infty$\;
$pr\left[i\right]\leftarrow\frac{1}{n}$\;
}
\emph{// Iterations}\;
\While{$|pr-pr'|\geq\epsilon$}{
$pr' \leftarrow pr$\;
\For{$i \leftarrow 1$ \KwTo $n$}{
$pr\left[i\right]\leftarrow\frac{1-d}{N}+d\sum_{j\in M\left(i\right)}\frac{pr'[j]}{L\left(j\right)}$\;
}
\emph{// Renormalization}\;
$s \leftarrow 0$\;
\For{$i\leftarrow 1$ \KwTo $n$}{
$s \leftarrow s + pr\left[i\right]$\;
}
\For{$i\leftarrow 1$ \KwTo $n$}{
$pr\left[i\right] \leftarrow pr\left[i\right] + \frac{1-s}{n}$\;
}
}
\BlankLine
\caption{Power method}\label{algo_power_method}
\end{algorithm}
\DecMargin{1em}

\section{Results}

\subsection{Influence of the damping factor}
Bla, bla, bla...

\begin{table}[h]
\centering
\begin{tabular}{ | r | l | }
\hline
1 & \href{http://rm.wikipedia.org/wiki/Special~Categories_101d.html}{Special~Categories\_101d.html} \\ \hline
2 & \href{http://rm.wikipedia.org/wiki/Germania.html}{Germania.html} \\ \hline
3 & \href{http://rm.wikipedia.org/wiki/Svizra.html}{Svizra.html} \\ \hline
4 & \href{http://rm.wikipedia.org/wiki/Category~Stadis_da_l'Europa_dfba.html}{Category~Stadis\_da\_l'Europa\_dfba.html} \\ \hline
5 & \href{http://rm.wikipedia.org/wiki/Europa.html}{Europa.html} \\ \hline
6 & \href{http://rm.wikipedia.org/wiki/Russia.html}{Russia.html} \\ \hline
7 & \href{http://rm.wikipedia.org/wiki/Sinsheim.html}{Sinsheim.html} \\ \hline
8 & \href{http://rm.wikipedia.org/wiki/Atletica.html}{Atletica.html} \\ \hline
9 & \href{http://rm.wikipedia.org/wiki/Baden-Württemberg_3684.html}{Baden-Württemberg\_3684.html} \\ \hline
10 & \href{http://rm.wikipedia.org/wiki/Chantun_Argovia_0958.html}{Chantun\_Argovia\_0958.html} \\ \hline
\end{tabular}
\caption{TODO: complete}
\end{table}


\subsection{The intentional surfer model}
Bla, bla, bla...


\section{Problems and solutions}

The first main problem encounters with the implementation of a search engine is the data fetching. Initially the designed architecture contained a \emph{web crawler}, a software application that browse systematically the HTML pages over the Internet, however efficient crawling is hard.
There are two main causes to this difficulty:
\begin{itemize}
\item Internet access gives rarely a high data rate for download
\item Some websites have a policy which limits the number of downloaded pages per second
\end{itemize}
The solution was to consider copies of some set of HTML pages and work with the computer file system.

An other problem with the data was the size of the set of HTML pages. Two limitations were encountered:
\begin{itemize}
\item Memory size gives an upper bound for the amount of data
\item PageRank prompts to collect as many data as possible since the algorithm takes advantage of the hyperlink structure (i.e. a restrict set of pages may correspond to some ignored links)
\end{itemize}
There is no solution to avoid these limitations without a financial contribution. Hence data have been choose under these constraints.

The memory problem affects both the input data and the internal data. The difficulty recurs with the choice of the data structures, for example the algebraic method with the use of the adjacency matrix must reach this limit rapidly. Furthermore the element which uses the highest amount of memory is the index, two constraints appear:
\begin{itemize}
\item Memory size again
\item Search engine prompts to keep as many details as possible for each page about the words it contains in order to give relevant results. The quality of the information retrieval depends on the amount of details kept.
\end{itemize}
The conclusion is the same as the previous problem.


\section{Conclusion}

Eventually a little search engine which works on a set of HTML pages using the PageRank algorithm have been implemented with success. Despite some inherent difficulties about memory, the project has succeed to apprehend the effect of the damping factor. It also shows up the sensibility of a search engine depending from its implementation.


\end{document}
